"""
ì„œìš¸ ì•„íŒŒíŠ¸ ë°ì´í„° í¬ë¡¤ëŸ¬
ê³µê³µë°ì´í„°í¬í„¸ APIì™€ ë„¤ì´ë²„ ë¶€ë™ì‚° í¬ë¡¤ë§ì„ ê²°í•©
"""
import requests
import pandas as pd
import time
import json
import urllib.parse
from bs4 import BeautifulSoup
from typing import List, Dict, Optional
from config import (
    PUBLIC_DATA_API_KEY, 
    SEOUL_DATA_API_KEY,
    SEOUL_REAL_ESTATE_DATASET_ID,
    SEOUL_APARTMENT_INFO_DATASET_ID,
    SEOUL_DISTRICTS, 
    CRAWL_DELAY
)
from utils import extract_district, calculate_pyeong, calculate_distance_to_subway


class SeoulApartmentCrawler:
    """ì„œìš¸ ì•„íŒŒíŠ¸ ë°ì´í„° í¬ë¡¤ëŸ¬"""
    
    def __init__(self):
        # API í‚¤ê°€ ì´ë¯¸ URL ì¸ì½”ë”©ë˜ì–´ ìˆìœ¼ë©´ ë””ì½”ë”©
        self.api_key = PUBLIC_DATA_API_KEY
        if self.api_key and self.api_key != "YOUR_API_KEY_HERE":
            try:
                # ì´ë¯¸ ì¸ì½”ë”©ëœ í‚¤ë¥¼ ë””ì½”ë”© (requestsê°€ ìë™ ì¸ì½”ë”©í•˜ë¯€ë¡œ)
                decoded = urllib.parse.unquote(self.api_key)
                self.api_key = decoded
            except:
                pass
        
        # ì„œìš¸ ì—´ë¦°ë°ì´í„°ê´‘ì¥ API í‚¤
        self.seoul_api_key = SEOUL_DATA_API_KEY
        self.seoul_dataset_id = SEOUL_REAL_ESTATE_DATASET_ID
        self.seoul_apartment_info_dataset_id = SEOUL_APARTMENT_INFO_DATASET_ID
        
        # ê³µê³µë°ì´í„°í¬í„¸ êµ­í† êµí†µë¶€ ì•„íŒŒíŠ¸ ì‹¤ê±°ë˜ê°€ API (HTTPS ì‚¬ìš©)
        self.base_url = "https://openapi.molit.go.kr/OpenAPI_ToolInstallPackage/service/rest/RTMSOBJSvc/getRTMSDataSvcAptRent"
        
        # ì„œìš¸ ì—´ë¦°ë°ì´í„°ê´‘ì¥ API ì—”ë“œí¬ì¸íŠ¸
        self.seoul_api_base = "http://openapi.seoul.go.kr:8088"
        self.data = []
    
    def test_api_key(self) -> bool:
        """
        API í‚¤ê°€ ì˜¬ë°”ë¥´ê²Œ ì„¤ì •ë˜ì–´ ìˆëŠ”ì§€ í…ŒìŠ¤íŠ¸
        
        Returns:
            bool: API í‚¤ê°€ ìœ íš¨í•˜ë©´ True
        """
        if not self.api_key or self.api_key == "YOUR_API_KEY_HERE":
            print("âŒ API í‚¤ê°€ ì„¤ì •ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")
            print("   config.py íŒŒì¼ì—ì„œ PUBLIC_DATA_API_KEYë¥¼ í™•ì¸í•˜ì„¸ìš”.")
            return False
        
        print(f"ğŸ“ API í‚¤ í™•ì¸: {self.api_key[:20]}... (ì²˜ìŒ 20ì)")
        
        # ê°„ë‹¨í•œ í…ŒìŠ¤íŠ¸ ìš”ì²­ (ì„œìš¸ì‹œ ê°•ë‚¨êµ¬, 2024ë…„ 1ì›”)
        try:
            # ê³µê³µë°ì´í„°í¬í„¸ APIëŠ” serviceKeyë¥¼ ì¿¼ë¦¬ íŒŒë¼ë¯¸í„°ë¡œ ì§ì ‘ ì „ë‹¬
            # URLì— ì§ì ‘ í¬í•¨ì‹œí‚¤ëŠ” ë°©ì‹ë„ ê°€ëŠ¥
            test_url = f"{self.base_url}?serviceKey={urllib.parse.quote(self.api_key)}&LAWD_CD=11680&DEAL_YMD=202401"
            
            response = requests.get(test_url, timeout=10)
            
            if response.status_code == 200:
                # XML ì‘ë‹µ í™•ì¸
                if "SERVICE_KEY_IS_NOT_REGISTERED_ERROR" in response.text:
                    print("âŒ API í‚¤ê°€ ë“±ë¡ë˜ì§€ ì•Šì•˜ê±°ë‚˜ ì˜ëª»ë˜ì—ˆìŠµë‹ˆë‹¤.")
                    print("   ê³µê³µë°ì´í„°í¬í„¸ì—ì„œ API í‚¤ë¥¼ í™•ì¸í•˜ì„¸ìš”.")
                    return False
                elif "NODATA_ERROR" in response.text:
                    print("âœ… API í‚¤ëŠ” ìœ íš¨í•˜ì§€ë§Œ í•´ë‹¹ ê¸°ê°„ì— ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤.")
                    print("   (ì´ëŠ” ì •ìƒì…ë‹ˆë‹¤ - API í‚¤ëŠ” ì˜¬ë°”ë¥´ê²Œ ì„¤ì •ë˜ì—ˆìŠµë‹ˆë‹¤)")
                    return True
                else:
                    print("âœ… API í‚¤ê°€ ì˜¬ë°”ë¥´ê²Œ ì„¤ì •ë˜ì–´ ìˆìŠµë‹ˆë‹¤.")
                    print(f"   ì‘ë‹µ ìƒíƒœ: {response.status_code}")
                    return True
            else:
                print(f"âš ï¸ API ì‘ë‹µ ì˜¤ë¥˜: {response.status_code}")
                print(f"   ì‘ë‹µ ë‚´ìš©: {response.text[:300]}")
                return False
        except requests.exceptions.ConnectionError:
            print("âŒ ë„¤íŠ¸ì›Œí¬ ì—°ê²° ì˜¤ë¥˜: API ì„œë²„ì— ì—°ê²°í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
            print("   ì¸í„°ë„· ì—°ê²°ì„ í™•ì¸í•˜ê±°ë‚˜ ë°©í™”ë²½ ì„¤ì •ì„ í™•ì¸í•˜ì„¸ìš”.")
            return False
        except Exception as e:
            print(f"âŒ API í…ŒìŠ¤íŠ¸ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {type(e).__name__}")
            print(f"   ì˜¤ë¥˜ ë‚´ìš©: {str(e)[:200]}")
            return False
    
    def crawl_public_data(self, district: str, year: int = 2024) -> List[Dict]:
        """
        ê³µê³µë°ì´í„°í¬í„¸ì—ì„œ ì•„íŒŒíŠ¸ ì •ë³´ í¬ë¡¤ë§
        
        Args:
            district: ìì¹˜êµ¬ëª…
            year: ì—°ë„
        
        Returns:
            List[Dict]: ì•„íŒŒíŠ¸ ì •ë³´ ë¦¬ìŠ¤íŠ¸
        """
        apartments = []
        
        # ê³µê³µë°ì´í„° APIëŠ” ì‹¤ì œë¡œëŠ” ì‹¤ê±°ë˜ê°€ ë°ì´í„°ë¥¼ ì œê³µí•˜ë¯€ë¡œ
        # ì—¬ê¸°ì„œëŠ” ìƒ˜í”Œ ë°ì´í„° êµ¬ì¡°ë¥¼ ë§Œë“¤ê³ , ì‹¤ì œë¡œëŠ” ë„¤ì´ë²„ ë¶€ë™ì‚°ì„ í¬ë¡¤ë§í•˜ëŠ” ë°©ì‹ ì‚¬ìš©
        print(f"{district} ë°ì´í„° ìˆ˜ì§‘ ì¤‘...")
        time.sleep(CRAWL_DELAY)
        
        return apartments
    
    def crawl_seoul_real_estate(self, start_index: int = 1, end_index: int = 1000) -> pd.DataFrame:
        """
        ì„œìš¸ ì—´ë¦°ë°ì´í„°ê´‘ì¥ì—ì„œ ë¶€ë™ì‚° ì‹¤ê±°ë˜ê°€ ë°ì´í„° í¬ë¡¤ë§
        ë°ì´í„°ì…‹ ID: OA-21275
        
        âš ï¸ API ì‚¬ìš© ì œí•œ:
        - í•˜ë£¨ ìµœëŒ€ 1,000íšŒ ìš”ì²­ ê°€ëŠ¥
        - 1íšŒì— ìµœëŒ€ 1,000ê±´ ìš”ì²­ ê°€ëŠ¥
        - 1,000ê±´ ì´ìƒì€ ë‚˜ëˆ„ì–´ì„œ í˜¸ì¶œ í•„ìš”
        
        Args:
            start_index: ì‹œì‘ ì¸ë±ìŠ¤
            end_index: ì¢…ë£Œ ì¸ë±ìŠ¤ (ìµœëŒ€ 1000ê°œì”© ì¡°íšŒ ê°€ëŠ¥)
        
        Returns:
            pd.DataFrame: ë¶€ë™ì‚° ì‹¤ê±°ë˜ê°€ ë°ì´í„°í”„ë ˆì„
        """
        """
        ì„œìš¸ ì—´ë¦°ë°ì´í„°ê´‘ì¥ì—ì„œ ë¶€ë™ì‚° ì‹¤ê±°ë˜ê°€ ë°ì´í„° í¬ë¡¤ë§
        ë°ì´í„°ì…‹ ID: OA-21275
        
        Args:
            start_index: ì‹œì‘ ì¸ë±ìŠ¤
            end_index: ì¢…ë£Œ ì¸ë±ìŠ¤ (ìµœëŒ€ 1000ê°œì”© ì¡°íšŒ ê°€ëŠ¥)
        
        Returns:
            pd.DataFrame: ë¶€ë™ì‚° ì‹¤ê±°ë˜ê°€ ë°ì´í„°í”„ë ˆì„
        """
        if self.seoul_api_key == "YOUR_SEOUL_API_KEY_HERE":
            print("âš ï¸ ì„œìš¸ ì—´ë¦°ë°ì´í„°ê´‘ì¥ API í‚¤ê°€ ì„¤ì •ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")
            print("   config.pyì—ì„œ SEOUL_DATA_API_KEYë¥¼ ì„¤ì •í•˜ì„¸ìš”.")
            print("   ë˜ëŠ” CSV íŒŒì¼ì„ ì§ì ‘ ë‹¤ìš´ë¡œë“œí•˜ì—¬ ì‚¬ìš©í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.")
            return pd.DataFrame()
        
        try:
            # ì„œìš¸ ì—´ë¦°ë°ì´í„°ê´‘ì¥ Open API ì—”ë“œí¬ì¸íŠ¸
            # í˜•ì‹: http://openapi.seoul.go.kr:8088/{ì¸ì¦í‚¤}/json/{ì„œë¹„ìŠ¤ëª…}/{ì‹œì‘ì¸ë±ìŠ¤}/{ì¢…ë£Œì¸ë±ìŠ¤}
            # âš ï¸ ì£¼ì˜: 1íšŒì— ìµœëŒ€ 1,000ê±´ë§Œ ìš”ì²­ ê°€ëŠ¥
            if end_index - start_index + 1 > 1000:
                print(f"âš ï¸ 1íšŒ ìš”ì²­ì€ ìµœëŒ€ 1,000ê±´ê¹Œì§€ ê°€ëŠ¥í•©ë‹ˆë‹¤. (ìš”ì²­: {end_index - start_index + 1}ê±´)")
                end_index = start_index + 999
            
            url = f"{self.seoul_api_base}/{self.seoul_api_key}/json/tbLnOpendataRentV/{start_index}/{end_index}"
            
            print(f"ì„œìš¸ ì—´ë¦°ë°ì´í„°ê´‘ì¥ API í˜¸ì¶œ ì¤‘... (ì¸ë±ìŠ¤: {start_index}~{end_index})")
            response = requests.get(url, timeout=30)
            
            if response.status_code == 200:
                data = response.json()
                
                # API ì‘ë‹µ êµ¬ì¡° í™•ì¸
                if 'tbLnOpendataRentV' in data:
                    result = data['tbLnOpendataRentV']
                    
                    if 'row' in result:
                        df = pd.DataFrame(result['row'])
                        print(f"âœ… {len(df)}ê°œì˜ ë°ì´í„°ë¥¼ ìˆ˜ì§‘í–ˆìŠµë‹ˆë‹¤.")
                        return df
                    else:
                        print("âš ï¸ ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤.")
                        return pd.DataFrame()
                else:
                    print(f"âš ï¸ API ì‘ë‹µ êµ¬ì¡°ê°€ ì˜ˆìƒê³¼ ë‹¤ë¦…ë‹ˆë‹¤: {list(data.keys())}")
                    return pd.DataFrame()
            else:
                print(f"âŒ API í˜¸ì¶œ ì‹¤íŒ¨: {response.status_code}")
                print(f"   ì‘ë‹µ: {response.text[:200]}")
                return pd.DataFrame()
                
        except Exception as e:
            print(f"âŒ ì„œìš¸ ì—´ë¦°ë°ì´í„°ê´‘ì¥ í¬ë¡¤ë§ ì˜¤ë¥˜: {type(e).__name__}")
            print(f"   ì˜¤ë¥˜ ë‚´ìš©: {str(e)[:200]}")
            return pd.DataFrame()
    
    def crawl_seoul_real_estate_all(self, max_records: int = 10000) -> pd.DataFrame:
        """
        ì„œìš¸ ì—´ë¦°ë°ì´í„°ê´‘ì¥ì—ì„œ ëª¨ë“  ë¶€ë™ì‚° ì‹¤ê±°ë˜ê°€ ë°ì´í„° í¬ë¡¤ë§
        (ì—¬ëŸ¬ ë²ˆ í˜¸ì¶œí•˜ì—¬ ì „ì²´ ë°ì´í„° ìˆ˜ì§‘)
        
        Args:
            max_records: ìµœëŒ€ ìˆ˜ì§‘í•  ë ˆì½”ë“œ ìˆ˜
        
        Returns:
            pd.DataFrame: ì „ì²´ ë¶€ë™ì‚° ì‹¤ê±°ë˜ê°€ ë°ì´í„°í”„ë ˆì„
        """
        all_data = []
        start_index = 1
        batch_size = 1000
        
        print(f"ì„œìš¸ ì—´ë¦°ë°ì´í„°ê´‘ì¥ì—ì„œ ìµœëŒ€ {max_records}ê°œì˜ ë°ì´í„°ë¥¼ ìˆ˜ì§‘í•©ë‹ˆë‹¤...")
        
        while start_index <= max_records:
            end_index = min(start_index + batch_size - 1, max_records)
            df_batch = self.crawl_seoul_real_estate(start_index, end_index)
            
            if df_batch.empty:
                print("ë” ì´ìƒ ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤.")
                break
            
            all_data.append(df_batch)
            start_index = end_index + 1
            
            # API í˜¸ì¶œ ì œí•œì„ ìœ„í•œ ì§€ì—°
            time.sleep(CRAWL_DELAY)
            
            if len(df_batch) < batch_size:
                print("ë§ˆì§€ë§‰ ë°°ì¹˜ë¥¼ ìˆ˜ì§‘í–ˆìŠµë‹ˆë‹¤.")
                break
        
        if all_data:
            result_df = pd.concat(all_data, ignore_index=True)
            print(f"\nâœ… ì´ {len(result_df)}ê°œì˜ ë°ì´í„°ë¥¼ ìˆ˜ì§‘í–ˆìŠµë‹ˆë‹¤.")
            return result_df
        else:
            print("âŒ ìˆ˜ì§‘ëœ ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤.")
            return pd.DataFrame()
    
    def process_seoul_real_estate_data(self, df: pd.DataFrame) -> pd.DataFrame:
        """
        ì„œìš¸ ì—´ë¦°ë°ì´í„°ê´‘ì¥ì—ì„œ ìˆ˜ì§‘í•œ ë°ì´í„°ë¥¼ ì•±ì—ì„œ ì‚¬ìš©í•  í˜•ì‹ìœ¼ë¡œ ë³€í™˜
        
        Args:
            df: ì›ë³¸ ë°ì´í„°í”„ë ˆì„
        
        Returns:
            pd.DataFrame: ë³€í™˜ëœ ë°ì´í„°í”„ë ˆì„
        """
        if df.empty:
            return df
        
        processed_data = []
        
        for _, row in df.iterrows():
            # ìì¹˜êµ¬ ì¶”ì¶œ
            district = extract_district(str(row.get('SGG_CD', '')) + str(row.get('BJDONG_NM', '')))
            if not district:
                # ì£¼ì†Œì—ì„œ ìì¹˜êµ¬ ì¶”ì¶œ ì‹œë„
                address = str(row.get('SGG_NM', '')) + str(row.get('BJDONG_NM', ''))
                district = extract_district(address)
            
            # ê±´ì¶•ë…„ë„
            build_year = row.get('BUILD_YEAR', None)
            try:
                build_year = int(build_year) if pd.notna(build_year) else None
            except:
                build_year = None
            
            # ë©´ì  ì •ë³´
            area_sqm = row.get('RENT_AREA', None) or row.get('RENT_GBN', None)
            try:
                area_sqm = float(area_sqm) if pd.notna(area_sqm) else None
            except:
                area_sqm = None
            
            pyeong = calculate_pyeong(area_sqm) if area_sqm else None
            
            # ì£¼ì†Œ êµ¬ì„±
            address = f"ì„œìš¸íŠ¹ë³„ì‹œ {row.get('SGG_NM', '')} {row.get('BJDONG_NM', '')} {row.get('BLDG_NM', '')}"
            address = address.strip()
            
            # ì¢Œí‘œ ì •ë³´ (ìˆëŠ” ê²½ìš°)
            lat = row.get('LAT', None)
            lon = row.get('LNG', None) or row.get('LON', None)
            
            # ì§€í•˜ì² ì—­ ê±°ë¦¬ ê³„ì‚°
            nearest_station = None
            distance_km = None
            if lat and lon:
                try:
                    nearest_station, distance_km = calculate_distance_to_subway(float(lat), float(lon))
                except:
                    pass
            
            apartment = {
                "ìì¹˜êµ¬": district or row.get('SGG_NM', ''),
                "ì£¼ì†Œ": address,
                "ê±´ì¶•ì—°ë„": build_year,
                "ì„¸ëŒ€ìˆ˜": None,  # ì‹¤ê±°ë˜ê°€ ë°ì´í„°ì—ëŠ” ì„¸ëŒ€ìˆ˜ ì •ë³´ê°€ ì—†ì„ ìˆ˜ ìˆìŒ
                "ë³µë„ê³„ë‹¨ì‹": None,  # ì‹¤ê±°ë˜ê°€ ë°ì´í„°ì—ëŠ” ì´ ì •ë³´ê°€ ì—†ì„ ìˆ˜ ìˆìŒ
                "ì „ìš©ë©´ì _ì œê³±ë¯¸í„°": area_sqm,
                "í‰í˜•": pyeong,
                "ìœ„ë„": lat,
                "ê²½ë„": lon,
                "ê°€ì¥ê°€ê¹Œìš´ì§€í•˜ì² ì—­": nearest_station,
                "ì§€í•˜ì² ì—­ê±°ë¦¬_km": distance_km,
                # ì¶”ê°€ ì •ë³´
                "ë¬¼ê±´ê¸ˆì•¡": row.get('RENT_GTN', None),
                "ë³´ì¦ê¸ˆ": row.get('RENT_DEPOSIT', None),
                "ì›”ì„¸": row.get('RENT_FEE', None),
                "ì‹ ê³ ë…„ë„": row.get('CNTRCT_DE', None),
            }
            
            processed_data.append(apartment)
        
        return pd.DataFrame(processed_data)
    
    def crawl_seoul_apartment_info(self, start_index: int = 1, end_index: int = 1000) -> pd.DataFrame:
        """
        ì„œìš¸ ì—´ë¦°ë°ì´í„°ê´‘ì¥ì—ì„œ ê³µë™ì£¼íƒ ì•„íŒŒíŠ¸ ì •ë³´ í¬ë¡¤ë§
        ë°ì´í„°ì…‹ ID: OA-15818 (ì„œìš¸ì‹œ ê³µë™ì£¼íƒ ì•„íŒŒíŠ¸ ì •ë³´)
        
        ì´ ë°ì´í„°ì…‹ì—ëŠ” ë‹¤ìŒ ì •ë³´ê°€ í¬í•¨ë©ë‹ˆë‹¤:
        - ì•„íŒŒíŠ¸ëª…, ì£¼ì†Œ
        - ì¤€ê³µì¼ì (ê±´ì¶•ì—°ë„)
        - ì„¸ëŒ€íƒ€ì… (ë³µë„/ê³„ë‹¨ì‹ ì •ë³´)
        - ì—°ë©´ì , ê´€ë¦¬ë¹„ë¶€ê³¼ë©´ì 
        - ê±´ì„¤ì‚¬, ì‹œí–‰ì‚¬
        - ë‚œë°©ë°©ì‹ ë“±
        
        âš ï¸ API ì‚¬ìš© ì œí•œ:
        - í•˜ë£¨ ìµœëŒ€ 1,000íšŒ ìš”ì²­ ê°€ëŠ¥
        - 1íšŒì— ìµœëŒ€ 1,000ê±´ ìš”ì²­ ê°€ëŠ¥
        - 1,000ê±´ ì´ìƒì€ ë‚˜ëˆ„ì–´ì„œ í˜¸ì¶œ í•„ìš”
        
        Args:
            start_index: ì‹œì‘ ì¸ë±ìŠ¤
            end_index: ì¢…ë£Œ ì¸ë±ìŠ¤ (ìµœëŒ€ 1000ê°œì”© ì¡°íšŒ ê°€ëŠ¥)
        
        Returns:
            pd.DataFrame: ì•„íŒŒíŠ¸ ì •ë³´ ë°ì´í„°í”„ë ˆì„
        """
        if self.seoul_api_key == "YOUR_SEOUL_API_KEY_HERE":
            print("âš ï¸ ì„œìš¸ ì—´ë¦°ë°ì´í„°ê´‘ì¥ API í‚¤ê°€ ì„¤ì •ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")
            print("   config.pyì—ì„œ SEOUL_DATA_API_KEYë¥¼ ì„¤ì •í•˜ì„¸ìš”.")
            print("   ë˜ëŠ” CSV íŒŒì¼ì„ ì§ì ‘ ë‹¤ìš´ë¡œë“œí•˜ì—¬ ì‚¬ìš©í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.")
            return pd.DataFrame()
        
        try:
            # ì„œìš¸ ì—´ë¦°ë°ì´í„°ê´‘ì¥ Open API ì—”ë“œí¬ì¸íŠ¸
            # í˜•ì‹: http://openapi.seoul.go.kr:8088/{ì¸ì¦í‚¤}/json/{ì„œë¹„ìŠ¤ëª…}/{ì‹œì‘ì¸ë±ìŠ¤}/{ì¢…ë£Œì¸ë±ìŠ¤}
            # ì„œë¹„ìŠ¤ëª…ì€ ë°ì´í„°ì…‹ì— ë”°ë¼ ë‹¤ë¥¼ ìˆ˜ ìˆìŒ (ì¼ë°˜ì ìœ¼ë¡œ ë°ì´í„°ì…‹ ID ê¸°ë°˜)
            # âš ï¸ ì£¼ì˜: 1íšŒì— ìµœëŒ€ 1,000ê±´ë§Œ ìš”ì²­ ê°€ëŠ¥
            if end_index - start_index + 1 > 1000:
                print(f"âš ï¸ 1íšŒ ìš”ì²­ì€ ìµœëŒ€ 1,000ê±´ê¹Œì§€ ê°€ëŠ¥í•©ë‹ˆë‹¤. (ìš”ì²­: {end_index - start_index + 1}ê±´)")
                end_index = start_index + 999
            
            # ì„œë¹„ìŠ¤ëª…: OpenAptInfo (ì„œìš¸ì‹œ ê³µë™ì£¼íƒ ì•„íŒŒíŠ¸ ì •ë³´)
            # API ì—”ë“œí¬ì¸íŠ¸: http://openapi.seoul.go.kr:8088/{ì¸ì¦í‚¤}/json/OpenAptInfo/{ì‹œì‘}/{ì¢…ë£Œ}
            service_name = "OpenAptInfo"
            url = f"{self.seoul_api_base}/{self.seoul_api_key}/json/{service_name}/{start_index}/{end_index}"
            
            print(f"ì„œìš¸ ì—´ë¦°ë°ì´í„°ê´‘ì¥ ì•„íŒŒíŠ¸ ì •ë³´ API í˜¸ì¶œ ì¤‘... (ì¸ë±ìŠ¤: {start_index}~{end_index})")
            response = requests.get(url, timeout=30)
            
            if response.status_code == 200:
                data = response.json()
                
                # API ì‘ë‹µ êµ¬ì¡°: OpenAptInfo -> row
                if 'OpenAptInfo' in data:
                    result = data['OpenAptInfo']
                    
                    # ì´ ë°ì´í„° ê°œìˆ˜ í™•ì¸
                    total_count = result.get('list_total_count', 0)
                    if start_index == 1:
                        print(f"   ì „ì²´ ë°ì´í„°: {total_count}ê±´")
                    
                    if 'row' in result:
                        df = pd.DataFrame(result['row'])
                        print(f"âœ… {len(df)}ê°œì˜ ì•„íŒŒíŠ¸ ì •ë³´ë¥¼ ìˆ˜ì§‘í–ˆìŠµë‹ˆë‹¤.")
                        return df
                    else:
                        print("âš ï¸ ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤.")
                        return pd.DataFrame()
                else:
                    print(f"âš ï¸ API ì‘ë‹µ êµ¬ì¡°ê°€ ì˜ˆìƒê³¼ ë‹¤ë¦…ë‹ˆë‹¤: {list(data.keys())}")
                    return pd.DataFrame()
            else:
                print(f"âŒ API í˜¸ì¶œ ì‹¤íŒ¨: {response.status_code}")
                print(f"   ì‘ë‹µ: {response.text[:200]}")
                return pd.DataFrame()
                
        except Exception as e:
            print(f"âŒ ì„œìš¸ ì—´ë¦°ë°ì´í„°ê´‘ì¥ ì•„íŒŒíŠ¸ ì •ë³´ í¬ë¡¤ë§ ì˜¤ë¥˜: {type(e).__name__}")
            print(f"   ì˜¤ë¥˜ ë‚´ìš©: {str(e)[:200]}")
            return pd.DataFrame()
    
    def crawl_seoul_apartment_info_all(self, max_records: int = 10000) -> pd.DataFrame:
        """
        ì„œìš¸ ì—´ë¦°ë°ì´í„°ê´‘ì¥ì—ì„œ ëª¨ë“  ì•„íŒŒíŠ¸ ì •ë³´ ë°ì´í„° í¬ë¡¤ë§
        (ì—¬ëŸ¬ ë²ˆ í˜¸ì¶œí•˜ì—¬ ì „ì²´ ë°ì´í„° ìˆ˜ì§‘)
        
        Args:
            max_records: ìµœëŒ€ ìˆ˜ì§‘í•  ë ˆì½”ë“œ ìˆ˜
        
        Returns:
            pd.DataFrame: ì „ì²´ ì•„íŒŒíŠ¸ ì •ë³´ ë°ì´í„°í”„ë ˆì„
        """
        all_data = []
        start_index = 1
        batch_size = 1000
        
        print(f"ì„œìš¸ ì—´ë¦°ë°ì´í„°ê´‘ì¥ì—ì„œ ìµœëŒ€ {max_records}ê°œì˜ ì•„íŒŒíŠ¸ ì •ë³´ë¥¼ ìˆ˜ì§‘í•©ë‹ˆë‹¤...")
        
        while start_index <= max_records:
            end_index = min(start_index + batch_size - 1, max_records)
            df_batch = self.crawl_seoul_apartment_info(start_index, end_index)
            
            if df_batch.empty:
                print("ë” ì´ìƒ ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤.")
                break
            
            all_data.append(df_batch)
            start_index = end_index + 1
            
            # API í˜¸ì¶œ ì œí•œì„ ìœ„í•œ ì§€ì—°
            time.sleep(CRAWL_DELAY)
            
            if len(df_batch) < batch_size:
                print("ë§ˆì§€ë§‰ ë°°ì¹˜ë¥¼ ìˆ˜ì§‘í–ˆìŠµë‹ˆë‹¤.")
                break
        
        if all_data:
            result_df = pd.concat(all_data, ignore_index=True)
            print(f"\nâœ… ì´ {len(result_df)}ê°œì˜ ì•„íŒŒíŠ¸ ì •ë³´ë¥¼ ìˆ˜ì§‘í–ˆìŠµë‹ˆë‹¤.")
            return result_df
        else:
            print("âŒ ìˆ˜ì§‘ëœ ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤.")
            return pd.DataFrame()
    
    def process_seoul_apartment_info_data(self, df: pd.DataFrame) -> pd.DataFrame:
        """
        ì„œìš¸ ì—´ë¦°ë°ì´í„°ê´‘ì¥ì—ì„œ ìˆ˜ì§‘í•œ ì•„íŒŒíŠ¸ ì •ë³´ ë°ì´í„°ë¥¼ ì•±ì—ì„œ ì‚¬ìš©í•  í˜•ì‹ìœ¼ë¡œ ë³€í™˜
        
        Args:
            df: ì›ë³¸ ë°ì´í„°í”„ë ˆì„
        
        Returns:
            pd.DataFrame: ë³€í™˜ëœ ë°ì´í„°í”„ë ˆì„
        """
        if df.empty:
            return df
        
        processed_data = []
        
        for _, row in df.iterrows():
            # ëª…ì„¸ì„œì— ë”°ë¥¸ í•„ë“œ ë§¤í•‘
            # APT_NM: k-ì•„íŒŒíŠ¸ëª…
            apt_name = row.get('APT_NM', '') or ''
            
            # APT_RDN_ADDR: kaptë„ë¡œëª…ì£¼ì†Œ
            address = row.get('APT_RDN_ADDR', '') or ''
            
            # SGG_ADDR: ì£¼ì†Œ(ì‹œêµ°êµ¬) - ìì¹˜êµ¬
            district = row.get('SGG_ADDR', '') or ''
            
            # USE_APRV_YMD: k-ì‚¬ìš©ê²€ì‚¬ì¼-ì‚¬ìš©ìŠ¹ì¸ì¼ (ì¤€ê³µì¼ì)
            completion_date = row.get('USE_APRV_YMD', None)
            build_year = None
            if completion_date and pd.notna(completion_date):
                try:
                    # ë‚ ì§œ í˜•ì‹: "2003-12-26 00:00:00.0"
                    date_str = str(completion_date)
                    if len(date_str) >= 4:
                        build_year = int(date_str[:4])
                except:
                    pass
            
            # TNOHSH: k-ì „ì²´ì„¸ëŒ€ìˆ˜
            households = row.get('TNOHSH', None)
            try:
                households = int(households) if pd.notna(households) else None
            except:
                households = None
            
            # ROAD_TYPE: k-ë³µë„ìœ í˜• (ë³µë„ì‹, ê³„ë‹¨ì‹, í˜¼í•©ì‹)
            road_type = row.get('ROAD_TYPE', '') or ''
            hallway_type = None
            if road_type:
                if 'ë³µë„' in str(road_type):
                    hallway_type = "ë³µë„ì‹"
                elif 'ê³„ë‹¨' in str(road_type):
                    hallway_type = "ê³„ë‹¨ì‹"
                elif 'í˜¼í•©' in str(road_type):
                    hallway_type = "í˜¼í•©ì‹"
                else:
                    hallway_type = str(road_type)  # ì›ë³¸ ê°’ ìœ ì§€
            
            # RSDT_XUAR: k-ì£¼ê±°ì „ìš©ë©´ì  (ì œê³±ë¯¸í„°)
            area_sqm = row.get('RSDT_XUAR', None)
            try:
                area_sqm = float(area_sqm) if pd.notna(area_sqm) else None
            except:
                area_sqm = None
            
            pyeong = calculate_pyeong(area_sqm) if area_sqm else None
            
            # YCRD: ì¢Œí‘œY (ìœ„ë„), XCRD: ì¢Œí‘œX (ê²½ë„)
            lat = row.get('YCRD', None)  # ìœ„ë„
            lon = row.get('XCRD', None)  # ê²½ë„
            
            # ì§€í•˜ì² ì—­ ê±°ë¦¬ ê³„ì‚°
            nearest_station = None
            distance_km = None
            if lat and lon and pd.notna(lat) and pd.notna(lon):
                try:
                    nearest_station, distance_km = calculate_distance_to_subway(float(lat), float(lon))
                except:
                    pass
            
            # BLDR: k-ê±´ì„¤ì‚¬(ì‹œê³µì‚¬)
            builder = row.get('BLDR', '') or ''
            
            # DVLR: k-ì‹œí–‰ì‚¬
            developer = row.get('DVLR', '') or ''
            
            # MN_MTHD: k-ë‚œë°©ë°©ì‹
            heating_method = row.get('MN_MTHD', '') or ''
            
            # HMPG: k-í™ˆí˜ì´ì§€
            homepage = row.get('HMPG', '') or ''
            
            # ì„¸ëŒ€ë‹¹ í‰ê·  ì „ìš©ë©´ì  ê³„ì‚° (ì „ì²´ ë‹¨ì§€ ì „ìš©ë©´ì  / ì„¸ëŒ€ìˆ˜)
            avg_area_per_household = None
            avg_pyeong_per_household = None
            if area_sqm and households and households > 0:
                try:
                    avg_area_per_household = round(area_sqm / households, 2)
                    avg_pyeong_per_household = calculate_pyeong(avg_area_per_household)
                except:
                    pass
            
            # PRK_CNTOM: ì£¼ì°¨ëŒ€ìˆ˜
            parking_count = row.get('PRK_CNTOM', None)
            try:
                parking_count = int(parking_count) if pd.notna(parking_count) else None
            except:
                parking_count = None
            
            # ì„¸ëŒ€ë‹¹ ì£¼ì°¨ ë©´ ê°¯ìˆ˜ ê³„ì‚°
            parking_per_household = None
            if parking_count and households and households > 0:
                try:
                    parking_per_household = round(parking_count / households, 2)
                except:
                    pass
            
            # ì „ìš©ë©´ì ë³„ ì„¸ëŒ€í˜„í™© ì •ë³´
            # XUAR_HH_STTS60: k-ì „ìš©ë©´ì ë³„ì„¸ëŒ€í˜„í™©(60ã¡ì´í•˜)
            hh_60sqm = row.get('XUAR_HH_STTS60', None)
            try:
                hh_60sqm = float(hh_60sqm) if pd.notna(hh_60sqm) else None
            except:
                hh_60sqm = None
            
            # XUAR_HH_STTS85: k-ì „ìš©ë©´ì ë³„ì„¸ëŒ€í˜„í™©(60mÂ²~85mÂ²ì´í•˜)
            hh_85sqm = row.get('XUAR_HH_STTS85', None)
            try:
                hh_85sqm = float(hh_85sqm) if pd.notna(hh_85sqm) else None
            except:
                hh_85sqm = None
            
            # XUAR_HH_STTS135: k-85mÂ²~135mÂ²ì´í•˜
            hh_135sqm = row.get('XUAR_HH_STTS135', None)
            try:
                hh_135sqm = float(hh_135sqm) if pd.notna(hh_135sqm) else None
            except:
                hh_135sqm = None
            
            # ì›ë³¸ ë°ì´í„°ë¥¼ ëª¨ë‘ ë³´ì¡´í•˜ë©´ì„œ í•„ìš”í•œ íŒŒìƒë³€ìˆ˜ë§Œ ì¶”ê°€
            apartment = {
                # === íŒŒìƒ/ë³€í™˜ëœ ì»¬ëŸ¼ (ì•±ì—ì„œ ì‚¬ìš©í•˜ê¸° í¸í•œ í˜•ì‹) ===
                "ìì¹˜êµ¬": district,
                "ì£¼ì†Œ": address,
                "ì•„íŒŒíŠ¸ëª…": apt_name,
                "ê±´ì¶•ì—°ë„": build_year,
                "ì„¸ëŒ€ìˆ˜": households,
                "ë³µë„ê³„ë‹¨ì‹": hallway_type,
                
                # ë©´ì  ì •ë³´ (ì›ë³¸ + íŒŒìƒ)
                "ì „ìš©ë©´ì _ì œê³±ë¯¸í„°": area_sqm,  # ì „ì²´ ë‹¨ì§€ ì „ìš©ë©´ì  í•©ê³„ (ì›ë³¸)
                "í‰í˜•": pyeong,  # ì „ì²´ ë‹¨ì§€ í‰í˜• í•©ê³„ (íŒŒìƒ)
                "ì„¸ëŒ€ë‹¹í‰ê· ì „ìš©ë©´ì _ì œê³±ë¯¸í„°": avg_area_per_household,  # ì„¸ëŒ€ë‹¹ í‰ê·  ì „ìš©ë©´ì  (íŒŒìƒ)
                "ì„¸ëŒ€ë‹¹í‰ê· í‰í˜•": avg_pyeong_per_household,  # ì„¸ëŒ€ë‹¹ í‰ê·  í‰í˜• (íŒŒìƒ)
                
                # ì „ìš©ë©´ì ë³„ ì„¸ëŒ€í˜„í™©
                "ì „ìš©ë©´ì 60ã¡ì´í•˜_ì„¸ëŒ€ìˆ˜": hh_60sqm,
                "ì „ìš©ë©´ì 60_85ã¡_ì„¸ëŒ€ìˆ˜": hh_85sqm,
                "ì „ìš©ë©´ì 85_135ã¡_ì„¸ëŒ€ìˆ˜": hh_135sqm,
                
                # ì£¼ì°¨ ì •ë³´
                "ì£¼ì°¨ëŒ€ìˆ˜": parking_count,  # ì£¼ì°¨ ëŒ€ìˆ˜ (ì›ë³¸)
                "ì„¸ëŒ€ë‹¹ì£¼ì°¨ë©´ìˆ˜": parking_per_household,  # ì„¸ëŒ€ë‹¹ ì£¼ì°¨ ë©´ ê°¯ìˆ˜ (íŒŒìƒ)
                
                # ìœ„ì¹˜ ì •ë³´
                "ìœ„ë„": lat,
                "ê²½ë„": lon,
                "ê°€ì¥ê°€ê¹Œìš´ì§€í•˜ì² ì—­": nearest_station,
                "ì§€í•˜ì² ì—­ê±°ë¦¬_km": distance_km,
                
                # ì¶”ê°€ ì •ë³´
                "ê±´ì„¤ì‚¬": builder,
                "ì‹œí–‰ì‚¬": developer,
                "ë‚œë°©ë°©ì‹": heating_method,
                "í™ˆí˜ì´ì§€": homepage,
                
                # === ì›ë³¸ API ì‘ë‹µ ì»¬ëŸ¼ ëª¨ë‘ ë³´ì¡´ ===
                "ì›ë³¸_SN": row.get('SN', None),
                "ì›ë³¸_APT_CD": row.get('APT_CD', ''),
                "ì›ë³¸_APT_NM": row.get('APT_NM', ''),
                "ì›ë³¸_CMPX_CLSF": row.get('CMPX_CLSF', ''),  # ë‹¨ì§€ë¶„ë¥˜
                "ì›ë³¸_APT_STDG_ADDR": row.get('APT_STDG_ADDR', ''),  # ì§€ë²ˆì£¼ì†Œ
                "ì›ë³¸_APT_RDN_ADDR": row.get('APT_RDN_ADDR', ''),  # ë„ë¡œëª…ì£¼ì†Œ
                "ì›ë³¸_CTPV_ADDR": row.get('CTPV_ADDR', ''),  # ì‹œë„ì£¼ì†Œ
                "ì›ë³¸_SGG_ADDR": row.get('SGG_ADDR', ''),  # ì‹œêµ°êµ¬ì£¼ì†Œ
                "ì›ë³¸_EMD_ADDR": row.get('EMD_ADDR', ''),  # ìë©´ë™ì£¼ì†Œ
                "ì›ë³¸_DADDR": row.get('DADDR', ''),  # ìƒì„¸ì£¼ì†Œ
                "ì›ë³¸_RDN_ADDR": row.get('RDN_ADDR', ''),  # ë„ë¡œëª…
                "ì›ë³¸_ROAD_DADDR": row.get('ROAD_DADDR', ''),  # ë„ë¡œëª…ìƒì„¸ì£¼ì†Œ
                "ì›ë³¸_TELNO": row.get('TELNO', ''),
                "ì›ë³¸_FXNO": row.get('FXNO', ''),  # íŒ©ìŠ¤ë²ˆí˜¸
                "ì›ë³¸_APT_CMPX": row.get('APT_CMPX', ''),  # ì•„íŒŒíŠ¸ë‹¨ì§€
                "ì›ë³¸_APT_ATCH_FILE": row.get('APT_ATCH_FILE', ''),  # ì²¨ë¶€íŒŒì¼
                "ì›ë³¸_HH_TYPE": row.get('HH_TYPE', ''),  # ì„¸ëŒ€ìœ í˜•
                "ì›ë³¸_MNG_MTHD": row.get('MNG_MTHD', ''),  # ê´€ë¦¬ë°©ë²•
                "ì›ë³¸_ROAD_TYPE": row.get('ROAD_TYPE', ''),  # ë³µë„ìœ í˜•
                "ì›ë³¸_MN_MTHD": row.get('MN_MTHD', ''),  # ë‚œë°©ë°©ì‹
                "ì›ë³¸_WHOL_DONG_CNT": row.get('WHOL_DONG_CNT', None),  # ì „ì²´ë™ìˆ˜
                "ì›ë³¸_TNOHSH": row.get('TNOHSH', None),  # ì „ì²´ì„¸ëŒ€ìˆ˜
                "ì›ë³¸_BLDR": row.get('BLDR', ''),  # ê±´ì„¤ì‚¬
                "ì›ë³¸_DVLR": row.get('DVLR', ''),  # ì‹œí–‰ì‚¬
                "ì›ë³¸_USE_APRV_YMD": row.get('USE_APRV_YMD', ''),  # ì‚¬ìš©ìŠ¹ì¸ì¼
                "ì›ë³¸_GFA": row.get('GFA', None),  # ì—°ë©´ì 
                "ì›ë³¸_RSDT_XUAR": row.get('RSDT_XUAR', None),  # ì£¼ê±°ì „ìš©ë©´ì 
                "ì›ë³¸_MNCO_LEVY_AREA": row.get('MNCO_LEVY_AREA', None),  # ê´€ë¦¬ë¹„ë¶€ê³¼ë©´ì 
                "ì›ë³¸_XUAR_HH_STTS60": row.get('XUAR_HH_STTS60', None),  # ì „ìš©ë©´ì ë³„ì„¸ëŒ€í˜„í™©(60ã¡ì´í•˜)
                "ì›ë³¸_XUAR_HH_STTS85": row.get('XUAR_HH_STTS85', None),  # ì „ìš©ë©´ì ë³„ì„¸ëŒ€í˜„í™©(60ã¡~85ã¡ì´í•˜)
                "ì›ë³¸_XUAR_HH_STTS135": row.get('XUAR_HH_STTS135', None),  # 85ã¡~135ã¡ì´í•˜
                "ì›ë³¸_XUAR_HH_STTS136": row.get('XUAR_HH_STTS136', None),  # 135ã¡ì´ˆê³¼
                "ì›ë³¸_HMPG": row.get('HMPG', ''),  # í™ˆí˜ì´ì§€
                "ì›ë³¸_REG_YMD": row.get('REG_YMD', ''),  # ë“±ë¡ì¼ì
                "ì›ë³¸_MDFCN_YMD": row.get('MDFCN_YMD', ''),  # ìˆ˜ì •ì¼ì
                "ì›ë³¸_EPIS_MNG_NO": row.get('EPIS_MNG_NO', ''),  # ì—í”¼ì†Œë“œê´€ë¦¬ë²ˆí˜¸
                "ì›ë³¸_EPS_MNG_FORM": row.get('EPS_MNG_FORM', ''),  # ì—í”¼ì†Œë“œê´€ë¦¬í˜•íƒœ
                "ì›ë³¸_HH_ELCT_CTRT_MTHD": row.get('HH_ELCT_CTRT_MTHD', ''),  # ì„¸ëŒ€ì „ê¸°ê³„ì•½ë°©ë²•
                "ì›ë³¸_CLNG_MNG_FORM": row.get('CLNG_MNG_FORM', ''),  # ëƒ‰ë°©ê´€ë¦¬í˜•íƒœ
                "ì›ë³¸_BDAR": row.get('BDAR', None),  # ê±´ë¬¼ë©´ì 
                "ì›ë³¸_PRK_CNTOM": row.get('PRK_CNTOM', None),  # ì£¼ì°¨ëŒ€ìˆ˜
                "ì›ë³¸_SE_CD": row.get('SE_CD', ''),  # ì‹œì„¤ì½”ë“œ
                "ì›ë³¸_CMPX_APRV_DAY": row.get('CMPX_APRV_DAY', ''),  # ë‹¨ì§€ìŠ¹ì¸ì¼
                "ì›ë³¸_USE_YN": row.get('USE_YN', ''),  # ì‚¬ìš©ì—¬ë¶€
                "ì›ë³¸_MNCO_ULD_YN": row.get('MNCO_ULD_YN', ''),  # ê´€ë¦¬ì‚¬ë¬´ì†Œìœ ë¬´
                "ì›ë³¸_XCRD": row.get('XCRD', ''),  # ê²½ë„
                "ì›ë³¸_YCRD": row.get('YCRD', ''),  # ìœ„ë„
                "ì›ë³¸_CMPX_APLD_DAY": row.get('CMPX_APLD_DAY', ''),  # ë‹¨ì§€ì ìš©ì¼
            }
            
            processed_data.append(apartment)
        
        return pd.DataFrame(processed_data)
    
    def download_seoul_apartment_csv_selenium(self) -> str:
        """
        Seleniumì„ ì‚¬ìš©í•˜ì—¬ ì„œìš¸ ì—´ë¦°ë°ì´í„°ê´‘ì¥ì—ì„œ CSV íŒŒì¼ ìë™ ë‹¤ìš´ë¡œë“œ
        (ë¡œê·¸ì¸ í•„ìš” ì‹œ ì‘ë™í•˜ì§€ ì•Šì„ ìˆ˜ ìˆìŒ)
        
        Returns:
            str: ë‹¤ìš´ë¡œë“œëœ íŒŒì¼ ê²½ë¡œ ë˜ëŠ” None
        """
        try:
            from selenium import webdriver
            from selenium.webdriver.common.by import By
            from selenium.webdriver.support.ui import WebDriverWait
            from selenium.webdriver.support import expected_conditions as EC
            from selenium.webdriver.chrome.options import Options
            import os
            import time
            import glob
            
            print("=" * 60)
            print("Seleniumì„ ì‚¬ìš©í•œ ìë™ ë‹¤ìš´ë¡œë“œ ì‹œë„")
            print("=" * 60)
            
            # Chrome ì˜µì…˜ ì„¤ì •
            chrome_options = Options()
            chrome_options.add_argument('--headless')  # ë°±ê·¸ë¼ìš´ë“œ ì‹¤í–‰
            chrome_options.add_argument('--no-sandbox')
            chrome_options.add_argument('--disable-dev-shm-usage')
            
            # ë‹¤ìš´ë¡œë“œ ê²½ë¡œ ì„¤ì •
            download_dir = os.getcwd()
            prefs = {
                "download.default_directory": download_dir,
                "download.prompt_for_download": False,
                "download.directory_upgrade": True
            }
            chrome_options.add_experimental_option("prefs", prefs)
            
            print("Chrome ë“œë¼ì´ë²„ ì´ˆê¸°í™” ì¤‘...")
            try:
                from webdriver_manager.chrome import ChromeDriverManager
                from selenium.webdriver.chrome.service import Service
                service = Service(ChromeDriverManager().install())
                driver = webdriver.Chrome(service=service, options=chrome_options)
            except:
                # webdriver-managerê°€ ì—†ìœ¼ë©´ ê¸°ë³¸ ê²½ë¡œ ì‚¬ìš©
                driver = webdriver.Chrome(options=chrome_options)
            
            try:
                url = "https://data.seoul.go.kr/dataList/OA-15818/S/1/datasetView.do"
                print(f"í˜ì´ì§€ ì ‘ì†: {url}")
                driver.get(url)
                
                time.sleep(3)  # í˜ì´ì§€ ë¡œë“œ ëŒ€ê¸°
                
                # CSV ë‹¤ìš´ë¡œë“œ ë²„íŠ¼ ì°¾ê¸°
                print("CSV ë‹¤ìš´ë¡œë“œ ë²„íŠ¼ ì°¾ëŠ” ì¤‘...")
                
                # ì—¬ëŸ¬ ê°€ëŠ¥í•œ ì„ íƒì ì‹œë„
                selectors = [
                    "a[href*='csv']",
                    "a[href*='download']",
                    "button[onclick*='csv']",
                    ".download",
                    "#download",
                ]
                
                download_clicked = False
                for selector in selectors:
                    try:
                        element = WebDriverWait(driver, 5).until(
                            EC.presence_of_element_located((By.CSS_SELECTOR, selector))
                        )
                        print(f"ë‹¤ìš´ë¡œë“œ ìš”ì†Œ ë°œê²¬: {selector}")
                        element.click()
                        download_clicked = True
                        break
                    except:
                        continue
                
                if not download_clicked:
                    print("âš ï¸  ë‹¤ìš´ë¡œë“œ ë²„íŠ¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
                    print("   í˜ì´ì§€ ì†ŒìŠ¤ í™•ì¸ ì¤‘...")
                    page_source = driver.page_source
                    if 'csv' in page_source.lower() or 'download' in page_source.lower():
                        print("   CSV ê´€ë ¨ ìš”ì†Œê°€ í˜ì´ì§€ì— ìˆì§€ë§Œ ìë™ í´ë¦­ ì‹¤íŒ¨")
                    driver.quit()
                    return None
                
                # ë‹¤ìš´ë¡œë“œ ì™„ë£Œ ëŒ€ê¸°
                print("ë‹¤ìš´ë¡œë“œ ì™„ë£Œ ëŒ€ê¸° ì¤‘...")
                time.sleep(10)
                
                # ë‹¤ìš´ë¡œë“œëœ íŒŒì¼ ì°¾ê¸°
                downloaded_files = glob.glob(os.path.join(download_dir, "*.csv"))
                if downloaded_files:
                    # ê°€ì¥ ìµœê·¼ íŒŒì¼
                    latest_file = max(downloaded_files, key=os.path.getctime)
                    print(f"âœ… ë‹¤ìš´ë¡œë“œ ì™„ë£Œ: {latest_file}")
                    driver.quit()
                    return latest_file
                else:
                    print("âš ï¸  ë‹¤ìš´ë¡œë“œëœ íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
                    driver.quit()
                    return None
                    
            except Exception as e:
                print(f"âŒ ë‹¤ìš´ë¡œë“œ ì˜¤ë¥˜: {type(e).__name__}")
                print(f"   ì˜¤ë¥˜ ë‚´ìš©: {str(e)[:200]}")
                driver.quit()
                return None
                
        except ImportError:
            print("âŒ Seleniumì´ ì„¤ì¹˜ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")
            print("   ì„¤ì¹˜: pip install selenium")
            return None
        except Exception as e:
            print(f"âŒ Selenium ì´ˆê¸°í™” ì˜¤ë¥˜: {type(e).__name__}")
            print(f"   ì˜¤ë¥˜ ë‚´ìš©: {str(e)[:200]}")
            return None
    
    def download_seoul_apartment_csv(self) -> str:
        """
        ì„œìš¸ ì—´ë¦°ë°ì´í„°ê´‘ì¥ì—ì„œ ì•„íŒŒíŠ¸ ë©”íƒ€ë°ì´í„° CSV íŒŒì¼ ë‹¤ìš´ë¡œë“œ ì‹œë„
        (ì‹¤ì œë¡œëŠ” ë‹¤ìš´ë¡œë“œ ë§í¬ë¥¼ ì°¾ì•„ì„œ ì•ˆë‚´)
        
        Returns:
            str: ë‹¤ìš´ë¡œë“œëœ íŒŒì¼ ê²½ë¡œ ë˜ëŠ” None
        """
        import os
        import requests
        from bs4 import BeautifulSoup
        
        print("=" * 60)
        print("ì„œìš¸ ì—´ë¦°ë°ì´í„°ê´‘ì¥ CSV ë‹¤ìš´ë¡œë“œ ì‹œë„")
        print("=" * 60)
        
        url = "https://data.seoul.go.kr/dataList/OA-15818/S/1/datasetView.do"
        
        try:
            headers = {
                'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36'
            }
            response = requests.get(url, headers=headers, timeout=30)
            
            if response.status_code == 200:
                soup = BeautifulSoup(response.text, 'html.parser')
                
                # CSV ë‹¤ìš´ë¡œë“œ ë§í¬ ì°¾ê¸°
                # ì„œìš¸ ì—´ë¦°ë°ì´í„°ê´‘ì¥ì€ ë³´í†µ JavaScriptë¡œ ë‹¤ìš´ë¡œë“œ ì²˜ë¦¬
                # ì§ì ‘ ë‹¤ìš´ë¡œë“œ URLì„ ì°¾ê¸° ì–´ë ¤ìš¸ ìˆ˜ ìˆìŒ
                
                print("âš ï¸  ìë™ ë‹¤ìš´ë¡œë“œê°€ ì–´ë µìŠµë‹ˆë‹¤.")
                print("   ì„œìš¸ ì—´ë¦°ë°ì´í„°ê´‘ì¥ì€ ë¡œê·¸ì¸ì´ í•„ìš”í•˜ê±°ë‚˜")
                print("   JavaScriptë¡œ ë‹¤ìš´ë¡œë“œë¥¼ ì²˜ë¦¬í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.")
                print("\nğŸ“¥ ìˆ˜ë™ ë‹¤ìš´ë¡œë“œ ë°©ë²•:")
                print("   1. https://data.seoul.go.kr/dataList/OA-15818/S/1/datasetView.do ì ‘ì†")
                print("   2. 'íŒŒì¼ë‚´ë ¤ë°›ê¸°' ë˜ëŠ” 'CSV ë‹¤ìš´ë¡œë“œ' ë²„íŠ¼ í´ë¦­")
                print("   3. ë‹¤ìš´ë¡œë“œí•œ íŒŒì¼ì„ í”„ë¡œì íŠ¸ í´ë”ì— ì €ì¥")
                
                return None
            else:
                print(f"âŒ í˜ì´ì§€ ì ‘ì† ì‹¤íŒ¨: {response.status_code}")
                return None
                
        except Exception as e:
            print(f"âŒ ë‹¤ìš´ë¡œë“œ ì‹œë„ ì˜¤ë¥˜: {type(e).__name__}")
            print(f"   ì˜¤ë¥˜ ë‚´ìš©: {str(e)[:200]}")
            return None
    
    def crawl_seoul_apartment_info_from_web(self, max_pages: int = 100) -> pd.DataFrame:
        """
        ì„œìš¸ ì—´ë¦°ë°ì´í„°ê´‘ì¥ ì›¹ì‚¬ì´íŠ¸ì—ì„œ ì•„íŒŒíŠ¸ ë©”íƒ€ë°ì´í„° í¬ë¡¤ë§
        ë¯¸ë¦¬ë³´ê¸° í˜ì´ì§€ì—ì„œ ë°ì´í„°ë¥¼ ìˆ˜ì§‘í•©ë‹ˆë‹¤.
        
        âš ï¸ ì£¼ì˜: ì›¹ í¬ë¡¤ë§ì€ ì‚¬ì´íŠ¸ ì •ì±…ì„ í™•ì¸í•˜ê³  ì‚¬ìš©í•˜ì„¸ìš”.
        ë¯¸ë¦¬ë³´ê¸°ëŠ” ìµœëŒ€ 1,000ê±´ê¹Œì§€ ë…¸ì¶œë©ë‹ˆë‹¤.
        
        Args:
            max_pages: ìµœëŒ€ ìˆ˜ì§‘í•  í˜ì´ì§€ ìˆ˜
        
        Returns:
            pd.DataFrame: ì•„íŒŒíŠ¸ ì •ë³´ ë°ì´í„°í”„ë ˆì„
        """
        print("=" * 60)
        print("ì„œìš¸ ì—´ë¦°ë°ì´í„°ê´‘ì¥ ì›¹ í¬ë¡¤ë§ ì‹œì‘")
        print("=" * 60)
        print("âš ï¸  ì›¹ í¬ë¡¤ë§ì€ ì‚¬ì´íŠ¸ ì •ì±…ì„ í™•ì¸í•˜ê³  ì‚¬ìš©í•˜ì„¸ìš”.")
        print("    ë¯¸ë¦¬ë³´ê¸°ëŠ” ìµœëŒ€ 1,000ê±´ê¹Œì§€ ë…¸ì¶œë©ë‹ˆë‹¤.")
        print("    ì „ì²´ ë°ì´í„°ëŠ” CSV íŒŒì¼ ë‹¤ìš´ë¡œë“œë¥¼ ê¶Œì¥í•©ë‹ˆë‹¤.")
        print("=" * 60)
        
        all_data = []
        
        try:
            # ì„œìš¸ ì—´ë¦°ë°ì´í„°ê´‘ì¥ ë¯¸ë¦¬ë³´ê¸° API ì—”ë“œí¬ì¸íŠ¸ ì‹œë„
            # ì‹¤ì œ ì—”ë“œí¬ì¸íŠ¸ëŠ” ì‚¬ì´íŠ¸ êµ¬ì¡°ì— ë”°ë¼ ë‹¤ë¥¼ ìˆ˜ ìˆìŒ
            base_url = "https://data.seoul.go.kr/dataList/OA-15818/S/1/datasetView.do"
            
            # ëŒ€ì•ˆ: CSV íŒŒì¼ì´ ì´ë¯¸ ìˆë‹¤ë©´ ë¡œë“œ
            print("\nğŸ’¡ ê¶Œì¥: CSV íŒŒì¼ì„ ì§ì ‘ ë‹¤ìš´ë¡œë“œí•˜ì—¬ ì‚¬ìš©í•˜ì„¸ìš”:")
            print("   https://data.seoul.go.kr/dataList/OA-15818/S/1/datasetView.do")
            print("   í˜ì´ì§€ì—ì„œ 'íŒŒì¼ë‚´ë ¤ë°›ê¸°' ë˜ëŠ” 'CSV ë‹¤ìš´ë¡œë“œ' í´ë¦­")
            print("\n   ë‹¤ìš´ë¡œë“œí•œ íŒŒì¼ì„ load_seoul_csv_file()ë¡œ ë¡œë“œí•˜ì„¸ìš”.")
            
            return pd.DataFrame()
            
        except Exception as e:
            print(f"âŒ ì›¹ í¬ë¡¤ë§ ì˜¤ë¥˜: {type(e).__name__}")
            print(f"   ì˜¤ë¥˜ ë‚´ìš©: {str(e)[:200]}")
            return pd.DataFrame()
    
    def crawl_seoul_apartment_info_all_with_csv(self, csv_file_path: str = None) -> pd.DataFrame:
        """
        CSV íŒŒì¼ì„ ì‚¬ìš©í•˜ì—¬ ì•„íŒŒíŠ¸ ë©”íƒ€ë°ì´í„° ì „ì²´ ìˆ˜ì§‘
        CSV íŒŒì¼ì´ ì—†ìœ¼ë©´ ë‹¤ìš´ë¡œë“œ ì•ˆë‚´
        
        Args:
            csv_file_path: CSV íŒŒì¼ ê²½ë¡œ (Noneì´ë©´ ìë™ ê²€ìƒ‰)
        
        Returns:
            pd.DataFrame: ì²˜ë¦¬ëœ ì•„íŒŒíŠ¸ ì •ë³´ ë°ì´í„°í”„ë ˆì„
        """
        import os
        import glob
        
        print("=" * 60)
        print("CSV íŒŒì¼ì„ í†µí•œ ì•„íŒŒíŠ¸ ë©”íƒ€ë°ì´í„° ìˆ˜ì§‘")
        print("=" * 60)
        
        # CSV íŒŒì¼ ì°¾ê¸°
        if csv_file_path is None:
            # ì¼ë°˜ì ì¸ íŒŒì¼ëª… íŒ¨í„´ ê²€ìƒ‰
            possible_files = [
                "*.csv",
                "*ì•„íŒŒíŠ¸*.csv",
                "*apartment*.csv",
                "*OA-15818*.csv",
            ]
            
            found_files = []
            for pattern in possible_files:
                found_files.extend(glob.glob(pattern))
            
            if found_files:
                csv_file_path = found_files[0]
                print(f"âœ… CSV íŒŒì¼ ë°œê²¬: {csv_file_path}")
            else:
                print("âŒ CSV íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
                print("\nğŸ“¥ CSV íŒŒì¼ ë‹¤ìš´ë¡œë“œ ë°©ë²•:")
                print("   1. https://data.seoul.go.kr/dataList/OA-15818/S/1/datasetView.do ì ‘ì†")
                print("   2. 'íŒŒì¼ë‚´ë ¤ë°›ê¸°' ë˜ëŠ” 'CSV ë‹¤ìš´ë¡œë“œ' í´ë¦­")
                print("   3. ë‹¤ìš´ë¡œë“œí•œ íŒŒì¼ì„ í”„ë¡œì íŠ¸ í´ë”ì— ì €ì¥")
                print("   4. ë‹¤ì‹œ ì‹¤í–‰í•˜ê±°ë‚˜ load_seoul_csv_file()ë¡œ ì§ì ‘ ë¡œë“œ")
                return pd.DataFrame()
        
        if not os.path.exists(csv_file_path):
            print(f"âŒ íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {csv_file_path}")
            return pd.DataFrame()
        
        # CSV íŒŒì¼ ë¡œë“œ ë° ì²˜ë¦¬
        print(f"\nğŸ“‚ CSV íŒŒì¼ ë¡œë“œ ì¤‘: {csv_file_path}")
        df = self.load_seoul_csv_file(csv_file_path)
        
        if df.empty:
            print("âŒ CSV íŒŒì¼ì´ ë¹„ì–´ìˆê±°ë‚˜ ë¡œë“œí•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
            return pd.DataFrame()
        
        print(f"âœ… {len(df)}ê±´ì˜ ë°ì´í„°ë¥¼ ë¡œë“œí–ˆìŠµë‹ˆë‹¤.")
        
        # ë°ì´í„° ë³€í™˜
        print("\nğŸ”„ ë°ì´í„° ë³€í™˜ ì¤‘...")
        processed_df = self.process_seoul_apartment_info_data(df)
        
        if not processed_df.empty:
            print(f"âœ… ë³€í™˜ ì™„ë£Œ! {len(processed_df)}ê±´ì˜ ë°ì´í„°ê°€ ì²˜ë¦¬ë˜ì—ˆìŠµë‹ˆë‹¤.")
            
            # ì €ì¥
            output_file = "seoul_apartments_metadata.csv"
            self.save_to_csv(processed_df, output_file)
            
            print(f"\nğŸ’¾ ìµœì¢… ë°ì´í„° ì €ì¥: {output_file}")
            print(f"   ì´ {len(processed_df)}ê±´ì˜ ì•„íŒŒíŠ¸ ë©”íƒ€ë°ì´í„°")
            
            return processed_df
        else:
            print("âŒ ë°ì´í„° ë³€í™˜ ì‹¤íŒ¨")
            return pd.DataFrame()
    
    def crawl_naver_real_estate(self, district: str) -> List[Dict]:
        """
        ë„¤ì´ë²„ ë¶€ë™ì‚°ì—ì„œ ì•„íŒŒíŠ¸ ì •ë³´ í¬ë¡¤ë§ (ìƒ˜í”Œ êµ¬ì¡°)
        ì‹¤ì œ í¬ë¡¤ë§ì€ ì›¹ì‚¬ì´íŠ¸ êµ¬ì¡°ì— ë”°ë¼ ì¡°ì • í•„ìš”
        
        Args:
            district: ìì¹˜êµ¬ëª…
        
        Returns:
            List[Dict]: ì•„íŒŒíŠ¸ ì •ë³´ ë¦¬ìŠ¤íŠ¸
        """
        apartments = []
        
        # ì‹¤ì œ í¬ë¡¤ë§ ì½”ë“œëŠ” ë„¤ì´ë²„ ë¶€ë™ì‚°ì˜ robots.txtì™€ ì´ìš©ì•½ê´€ì„ í™•ì¸ í›„ ì‘ì„±
        # ì—¬ê¸°ì„œëŠ” ìƒ˜í”Œ ë°ì´í„° ìƒì„± êµ¬ì¡°ë§Œ ì œê³µ
        print(f"{district} ë„¤ì´ë²„ ë¶€ë™ì‚° ë°ì´í„° ìˆ˜ì§‘ ì¤‘...")
        time.sleep(CRAWL_DELAY)
        
        return apartments
    
    def generate_sample_data(self, num_samples: int = 100) -> pd.DataFrame:
        """
        ìƒ˜í”Œ ë°ì´í„° ìƒì„± (ì‹¤ì œ í¬ë¡¤ë§ ì „ í…ŒìŠ¤íŠ¸ìš©)
        
        Args:
            num_samples: ìƒì„±í•  ìƒ˜í”Œ ìˆ˜
        
        Returns:
            pd.DataFrame: ì•„íŒŒíŠ¸ ë°ì´í„°í”„ë ˆì„
        """
        import random
        
        districts = SEOUL_DISTRICTS
        hallway_types = ["ë³µë„ì‹", "ê³„ë‹¨ì‹", "í˜¼í•©ì‹"]
        
        sample_data = []
        
        for i in range(num_samples):
            district = random.choice(districts)
            build_year = random.randint(1980, 2024)
            households = random.randint(100, 2000)
            hallway_type = random.choice(hallway_types)
            
            # ì„œìš¸ì‹œ ë‚´ ëœë¤ ì¢Œí‘œ ìƒì„±
            lat = random.uniform(37.4, 37.7)
            lon = random.uniform(126.8, 127.2)
            
            # í‰í˜• ê³„ì‚° (ì „ìš©ë©´ì  ê¸°ì¤€)
            area_sqm = random.uniform(50, 150)
            pyeong = calculate_pyeong(area_sqm)
            
            # ì§€í•˜ì² ì—­ ê±°ë¦¬ ê³„ì‚°
            nearest_station, distance_km = calculate_distance_to_subway(lat, lon)
            
            apartment = {
                "ìì¹˜êµ¬": district,
                "ì£¼ì†Œ": f"ì„œìš¸íŠ¹ë³„ì‹œ {district} {random.choice(['ë¡œ', 'ê¸¸'])} {random.randint(1, 999)}",
                "ê±´ì¶•ì—°ë„": build_year,
                "ì„¸ëŒ€ìˆ˜": households,
                "ë³µë„ê³„ë‹¨ì‹": hallway_type,
                "ì „ìš©ë©´ì _ì œê³±ë¯¸í„°": round(area_sqm, 2),
                "í‰í˜•": pyeong,
                "ìœ„ë„": round(lat, 6),
                "ê²½ë„": round(lon, 6),
                "ê°€ì¥ê°€ê¹Œìš´ì§€í•˜ì² ì—­": nearest_station,
                "ì§€í•˜ì² ì—­ê±°ë¦¬_km": distance_km
            }
            
            sample_data.append(apartment)
        
        return pd.DataFrame(sample_data)
    
    def save_to_csv(self, df: pd.DataFrame, filename: str = "seoul_apartments.csv"):
        """
        ë°ì´í„°ë¥¼ CSV íŒŒì¼ë¡œ ì €ì¥
        
        Args:
            df: ì €ì¥í•  ë°ì´í„°í”„ë ˆì„
            filename: íŒŒì¼ëª…
        """
        df.to_csv(filename, index=False, encoding='utf-8-sig')
        print(f"ë°ì´í„°ê°€ {filename}ì— ì €ì¥ë˜ì—ˆìŠµë‹ˆë‹¤. (ì´ {len(df)}ê°œ)")
    
    def load_from_csv(self, filename: str = "seoul_apartments.csv") -> pd.DataFrame:
        """
        CSV íŒŒì¼ì—ì„œ ë°ì´í„° ë¡œë“œ
        
        Args:
            filename: íŒŒì¼ëª…
        
        Returns:
            pd.DataFrame: ë¡œë“œëœ ë°ì´í„°í”„ë ˆì„
        """
        try:
            df = pd.read_csv(filename, encoding='utf-8-sig')
            print(f"{filename}ì—ì„œ {len(df)}ê°œì˜ ë°ì´í„°ë¥¼ ë¡œë“œí–ˆìŠµë‹ˆë‹¤.")
            return df
        except FileNotFoundError:
            print(f"{filename} íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
            return pd.DataFrame()
    
    def load_seoul_csv_file(self, csv_file_path: str) -> pd.DataFrame:
        """
        ì„œìš¸ ì—´ë¦°ë°ì´í„°ê´‘ì¥ì—ì„œ ë‹¤ìš´ë¡œë“œí•œ CSV íŒŒì¼ì„ ë¡œë“œí•˜ê³  ì²˜ë¦¬
        
        Args:
            csv_file_path: CSV íŒŒì¼ ê²½ë¡œ
        
        Returns:
            pd.DataFrame: ì²˜ë¦¬ëœ ë°ì´í„°í”„ë ˆì„
        """
        try:
            print(f"CSV íŒŒì¼ ë¡œë“œ ì¤‘: {csv_file_path}")
            df = pd.read_csv(csv_file_path, encoding='utf-8-sig')
            print(f"âœ… {len(df)}ê°œì˜ ë°ì´í„°ë¥¼ ë¡œë“œí–ˆìŠµë‹ˆë‹¤.")
            
            # ë°ì´í„° ë³€í™˜
            processed_df = self.process_seoul_real_estate_data(df)
            
            return processed_df
        except FileNotFoundError:
            print(f"âŒ íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {csv_file_path}")
            return pd.DataFrame()
        except Exception as e:
            print(f"âŒ CSV íŒŒì¼ ë¡œë“œ ì˜¤ë¥˜: {type(e).__name__}")
            print(f"   ì˜¤ë¥˜ ë‚´ìš©: {str(e)[:200]}")
            return pd.DataFrame()


if __name__ == "__main__":
    crawler = SeoulApartmentCrawler()
    
    # API í‚¤ í…ŒìŠ¤íŠ¸
    print("=" * 50)
    print("API í‚¤ í…ŒìŠ¤íŠ¸ ì¤‘...")
    print("=" * 50)
    api_valid = crawler.test_api_key()
    print()
    
    if api_valid:
        print("ê³µê³µë°ì´í„° APIë¥¼ ì‚¬ìš©í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.")
    else:
        print("API í‚¤ì— ë¬¸ì œê°€ ìˆê±°ë‚˜, ìƒ˜í”Œ ë°ì´í„°ë¥¼ ì‚¬ìš©í•©ë‹ˆë‹¤.")
    
    print("\n" + "=" * 60)
    print("ì„œìš¸ ì—´ë¦°ë°ì´í„°ê´‘ì¥ ë°ì´í„° í¬ë¡¤ë§")
    print("=" * 60)
    print("\nğŸ“‹ ë°ì´í„°ì…‹ ì •ë³´:")
    print("  1. OA-15818: ì„œìš¸ì‹œ ê³µë™ì£¼íƒ ì•„íŒŒíŠ¸ ì •ë³´ (ë©”íƒ€ë°ì´í„°)")
    print("     - ì•„íŒŒíŠ¸ëª…, ì£¼ì†Œ, ì¤€ê³µì¼ì, ì„¸ëŒ€ìˆ˜, ì„¸ëŒ€íƒ€ì… ë“±")
    print("     - https://data.seoul.go.kr/dataList/OA-15818/S/1/datasetView.do")
    print("\n  2. OA-21275: ì„œìš¸ì‹œ ë¶€ë™ì‚° ì‹¤ê±°ë˜ê°€ ì •ë³´")
    print("     - ì‹¤ê±°ë˜ê°€, ë³´ì¦ê¸ˆ, ì›”ì„¸, ì‹ ê³ ë…„ë„ ë“±")
    print("     - https://data.seoul.go.kr/dataList/OA-21275/S/1/datasetView.do")
    print("\n" + "=" * 60)
    
    # 1. ì•„íŒŒíŠ¸ ë©”íƒ€ë°ì´í„° í¬ë¡¤ë§ ì‹œë„ (OA-15818)
    print("\n[1ë‹¨ê³„] ì•„íŒŒíŠ¸ ë©”íƒ€ë°ì´í„° ìˆ˜ì§‘ ì‹œë„ (OA-15818)...")
    apartment_info_df = crawler.crawl_seoul_apartment_info_all(max_records=5000)
    
    if not apartment_info_df.empty:
        # ë°ì´í„° ë³€í™˜
        processed_df = crawler.process_seoul_apartment_info_data(apartment_info_df)
        
        # CSVë¡œ ì €ì¥
        crawler.save_to_csv(processed_df, "seoul_apartments_metadata.csv")
        
        print("\nâœ… ì•„íŒŒíŠ¸ ë©”íƒ€ë°ì´í„° ìˆ˜ì§‘ ì™„ë£Œ!")
        print(f"   ì´ {len(processed_df)}ê°œì˜ ì•„íŒŒíŠ¸ ì •ë³´ê°€ ì €ì¥ë˜ì—ˆìŠµë‹ˆë‹¤.")
        print("\nìˆ˜ì§‘ëœ ë°ì´í„° ìƒ˜í”Œ:")
        print(processed_df.head())
        
        # ë©”íƒ€ë°ì´í„°ë¥¼ ë©”ì¸ ë°ì´í„°ë¡œ ì‚¬ìš©
        main_df = processed_df
    else:
        print("âš ï¸  ì•„íŒŒíŠ¸ ë©”íƒ€ë°ì´í„°ë¥¼ APIë¡œ ìˆ˜ì§‘í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
        print("   CSV íŒŒì¼ ë‹¤ìš´ë¡œë“œ ë°©ì‹ì„ ì‚¬ìš©í•˜ì„¸ìš”.")
        main_df = None
    
    # 2. ì‹¤ê±°ë˜ê°€ ë°ì´í„° í¬ë¡¤ë§ ì‹œë„ (OA-21275)
    print("\n" + "=" * 60)
    print("[2ë‹¨ê³„] ë¶€ë™ì‚° ì‹¤ê±°ë˜ê°€ ë°ì´í„° ìˆ˜ì§‘ ì‹œë„ (OA-21275)...")
    real_estate_df = crawler.crawl_seoul_real_estate_all(max_records=5000)
    
    if not real_estate_df.empty:
        # ë°ì´í„° ë³€í™˜
        processed_real_estate_df = crawler.process_seoul_real_estate_data(real_estate_df)
        
        # CSVë¡œ ì €ì¥
        crawler.save_to_csv(processed_real_estate_df, "seoul_real_estate.csv")
        
        print("\nâœ… ì‹¤ê±°ë˜ê°€ ë°ì´í„° ìˆ˜ì§‘ ì™„ë£Œ!")
        print(f"   ì´ {len(processed_real_estate_df)}ê°œì˜ ì‹¤ê±°ë˜ê°€ ì •ë³´ê°€ ì €ì¥ë˜ì—ˆìŠµë‹ˆë‹¤.")
        print("\nìˆ˜ì§‘ëœ ë°ì´í„° ìƒ˜í”Œ:")
        print(processed_real_estate_df.head())
    else:
        print("âš ï¸  ì‹¤ê±°ë˜ê°€ ë°ì´í„°ë¥¼ APIë¡œ ìˆ˜ì§‘í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
        print("   CSV íŒŒì¼ ë‹¤ìš´ë¡œë“œ ë°©ì‹ì„ ì‚¬ìš©í•˜ì„¸ìš”.")
    
    # 3. ìµœì¢… ë°ì´í„° í†µí•© ë˜ëŠ” ìƒ˜í”Œ ë°ì´í„° ìƒì„±
    if main_df is None or main_df.empty:
        print("\n" + "=" * 60)
        print("[3ë‹¨ê³„] ìƒ˜í”Œ ë°ì´í„° ìƒì„±...")
        print("=" * 60)
        print("\nâš ï¸  ì‹¤ì œ ë°ì´í„°ë¥¼ ìˆ˜ì§‘í•  ìˆ˜ ì—†ì–´ ìƒ˜í”Œ ë°ì´í„°ë¥¼ ìƒì„±í•©ë‹ˆë‹¤.")
        print("   CSV íŒŒì¼ì„ ë‹¤ìš´ë¡œë“œí•˜ì—¬ ì‚¬ìš©í•˜ì‹œë©´ ë” ì •í™•í•œ ë°ì´í„°ë¥¼ ì–»ì„ ìˆ˜ ìˆìŠµë‹ˆë‹¤.")
        print("   (CSV_DOWNLOAD_GUIDE.md ì°¸ê³ )")
        
        # ìƒ˜í”Œ ë°ì´í„° ìƒì„±
        df = crawler.generate_sample_data(num_samples=500)
        
        # CSVë¡œ ì €ì¥
        crawler.save_to_csv(df, "seoul_apartments.csv")
        
        print("\nâœ… ìƒ˜í”Œ ë°ì´í„° ìƒì„± ì™„ë£Œ!")
        print(f"   ì´ {len(df)}ê°œì˜ ìƒ˜í”Œ ë°ì´í„°ê°€ ì €ì¥ë˜ì—ˆìŠµë‹ˆë‹¤.")
        print("\nìƒì„±ëœ ë°ì´í„° ìƒ˜í”Œ:")
        print(df.head())
    else:
        # ë©”íƒ€ë°ì´í„°ë¥¼ ë©”ì¸ ë°ì´í„°ë¡œ ì‚¬ìš©
        crawler.save_to_csv(main_df, "seoul_apartments.csv")
        print("\nâœ… ìµœì¢… ë°ì´í„° ì €ì¥ ì™„ë£Œ: seoul_apartments.csv")

